\documentclass[12pt]{article}
\usepackage{color}
\begin{document}
\begin{titlepage}
\title{\textbf{CPSC 504 Project\\\vspace{3 cm}\Huge{An investigation into an application to manage data transformations and updates}\vspace{2 cm}}}
\date{\vspace{2 cm} April 14, 2015}

\author{
 \makebox[1.0\linewidth]{Laura Cang}\\cang@cs.ubc.ca\\
  \and \makebox[1.0\linewidth]{Kailang Jiang}\\jiangkl@cs.ubc.ca\\
  \and \makebox[1.0\linewidth]{Jessica Wong}\\jhmwong@cs.ubc.ca\\ 
}
\maketitle
\thispagestyle{empty}
\end{titlepage}

\newpage
\section{Introduction}
\label{sec:introduction}
This project is built on top of the theoretical framework done in the M.Sc thesis of Arni Thrastarson \cite{arniThesis} and focuses on the first of the four scenarios detailed in the work. The first scenario is about trying to investigate how changes made by a data application can be captured and propagated to a remote data source. The main workflow of this scenario would be a remote (possibly third party) application pulling the data from a data source to use locally. The application will receive a local copy of the data along with a schema mapping to identify which tables the columns in the local copy of the data came from. A user using the application may notice inconsistencies with the data and go to correct it. The changes made by this user are logged in a transformation script and when the time comes to update the data source, the transformation script is sent to the data store and the changes detailed in the script are applied.

In this project, we focus on three different ways a user can use the application to fetch data. For the rest of this paper, we shall call each way of data fetching a sub-scenario. 
\begin{enumerate}
	\item {When the user chooses to view all the columns in one table}
	\item {When the user chooses to view a subset of the columns in one table}
	\item {When the user chooses to view a subset of the columns in multiple tables}
\end{enumerate}

\noindent Our primary contributions are:
\begin{enumerate}
	\item{Using logical programming concepts to define a data transformation language that captures changes made to a relational data model}
	\item{Design decisions about how the relational model should react and handle schema changes}
	\item{A preliminary investigation into view maintenance policies for the remote datastore}
	\item{Further detailing the scenario in which an application views a subset of columns from one or more tables in a remote dataset}
	\item{Deciding which issues to assign to the server side and which to the client side}
\end{enumerate}

\section{Motivation}
In our project, our main focus is on a scenario where the user views any subset of columns from one or more tables, it is important to figure out how this relates to the real world. The example in \cite{arniThesis} talks about a sports reporter who retrieves select information about basketball players to do an article on. In this case, the reporter only takes out columns regarding a player's name, his high school, and the first team he played on after entering the NBA. Compared to all the information that is stored about NBA players on a main database (e.g., salary, weight, height, number of transfers, agent name, etc.), the reporter has only taken a very small subset of columns. Another example could be a school administrator who wants to decide which students to give awards to purely based on academic merit. He/she could go to the main database storing all student information (e.g., the database could have information regarding a student's name, student number, GPA, guardian contact information, date of birth, allergy/medication information, family doctor contact, etc.) and only choose to view the student number and GPA column so that he/she will not be biased towards any names or gender.

An example where a user might need to view a subset of columns in multiple tables would be if the user is trying to find detailed information about proteins found in different microbiological organisms. The microbiological organisms and the various proteins it produces could be stored in one table while detailed information about each protein is stored in another. If a user wanted to see certain characteristics of a protein found in specific organisms, he/she would have use a subset of columns from both the microbiological organism table along with the protein time to get all the relevant information.

\subsection{Motivation used in our project}
In our project, we decided to borrow the dataset used in CPSC 304, an introductory relational database course at UBC, to use as the test dataset in our application. This was a movie dataset that we decided to take three columns out of to use \textcolor{red}{put stuff here because i don't actually know anything about the dataset}

\section{Solution}
The basic workflow of the scenario described in Section \ref{sec:introduction} will remain the same. However, as the user obtains more flexibility over what columns can be chosen to view in his/her application,  the complexity of the data transformations increase along with the amount of information that must be tracked (see Section \ref{sec:transformation_language}). 

\subsection{Types of Changes Supported}
Based on the various classes of changes mentioned in Wrangler \cite{kandel2011wrangler}, we decided to focus on two of them: map and reshape. Map changes deal with "[transforming] one input data row to zero, one, or multiple output rows" \cite{kandel2011wrangler}. This essentially means that the row changes that are supported in our application (e.g., add row, delete row, edit value(s) in row(s)) all fall within this class of changes. Reshape changes deal with schema changes. Any change to the columns (e.g., add/remove/rename columns) will fall into this category. The scope of this project does not deal with changing the data type of a column.

\subsection{Transformation Language}
\label{sec:transformation_language}
As we designed the transformation language, we realized it did not make sense to work off the simplest of our three sub-scenarios as the transformation language did not adapt well when the complexity of the sub-scenario was increased. Hence, we decided to work off the most complicated of our three sub-scenarios, the situation where an application has pulled a subset of columns from more than one table, and design our transformation language from that point onwards. 

\textbf{\textcolor{green}{more here}}

\subsection{View Maintenance}
View maintenance \textcolor{red}{cite stuff here}

\subsection{Proof of Concept}
After designing the transformation language, we decided to implement the scenario workflow in order to show that our transformation language and design decisions could work to update a data source. The only extra step that was not included in \cite{arniThesis} was to allow for the user to choose which changes to sync to the remote data source. \textcolor{red}{add screen shots!! Kailang can write about all the cool things she did here!! :)}

\section{Related Work}
\textcolor{red}{need stuff here can probably reuse the stuff from the project update but will need more + editing}
\textbf{\textcolor{green}{Stuff in this section is ripped from the project update}}
One of the largest problems that we foresee is creating a transformation language to track all the possible data changes as well as how to propagate those changes correctly from one source to another. Hence, most of our related work focuses on systems that are similar or areas of work where we feel we can draw inspiration from how they tackled a similar problem to the one we are facing.

\subsection{Data Sharing Systems}
Data sharing situations are not new; there have been many collaborative data sharing systems (CDSS) \cite{ives2008orchestra, ives2005orchestra, karvounarakis2013collaborative}and data transformation language \cite{kandel2011wrangler, lakshmanan2001schemasql} created to try and address this issue. From CDSS systems, we believe we can examine how it handles schema mapping change propagations to inform our own design as this is exactly the same problem we are facing. 

\subsection{Transformation Languages}
There is no lack of data transformation tools 

\subsection{View Maintenance}
Currently we have chosen to only allow data from a single table be viewed in our interface. However, given that this project could be extended in the future to include viewing data that is from multiple tables, we are looking ahead at literature in the view maintenance area to try and predict what challenges our interface might face \cite{ agrawal1997efficient, agrawal2009asynchronous, gupta1999materialized, zhou2007lazy}. We also thought that we might be able to get some ideas on how often to push any possible schema mapping/schema changes as it seems to be a comparable problem to how materialized views need to push its changes to a remote database \cite{agrawal1997efficient, agrawal2009asynchronous, zhou2007lazy}. 


\section{Future Work}
Future extensions of this work can look into testing the transformation language on other data formats like XML to see if it can effectively capture the changes that happen in an XML document. We can also look into how to handle situations where there are multiple concurrent users drawing upon the same data source and what policies should be adopted into order to achieve the best balance between performance and data consistency. Updating the data source too often could cause performance overhead from locking up the data source while updating too infrequently could cause inconsistent data to be propagated to many different users. The amount of effort and operational overhead involved with many people making the same changes would be undesirable. 

Another extension of the project could be investigating how the data transformations could be integrated into data provenance to help with data conflict resolutions \cite{arniThesis}. As the project currently stands, the remote application can push any type of data to the data source. If the data being pushed to the data source violates any table constraints (e.g., if a user tries to push a non-unique value into a column that has specified that it only takes unique values), the database administrator has to manually resolve that violation. It is possible to investigate whether or not provenance information can be leveraged to help make these decisions thus lowering the workload on the database administrator. A possible example of how to use the provenance information could be using the previous history of the sorts of changes have been accepted for that column or table to determine the likelihood of the current data violation being accepted. Provenance could also be used to determine the likelihood of correctness by the app or user who made the change; as information about who has changed values can be tracked, it is not inconceivable that the statistics of who has made the best or the most correct/acceptable changes can be used in some way.

\bibliographystyle{abbrv}
\bibliography{final_paper_biblography}

\end{document}